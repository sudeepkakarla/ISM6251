{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccf96132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d4ebc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('./airbnb_train_X_price_gte_150.csv') \n",
    "y_train = pd.read_csv('./airbnb_train_y_price_gte_150.csv') \n",
    "X_test = pd.read_csv('./airbnb_test_X_price_gte_150.csv') \n",
    "y_test = pd.read_csv('./airbnb_test_y_price_gte_150.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76e9f05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.DataFrame({\"model\": [], \"Accuracy\": [], \"Precision\": [], \"Recall\": [], \"F1\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d22401a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "The best precision score is 0.8584728930092839\n",
      "... with parameters: {'min_samples_split': 53, 'min_samples_leaf': 19, 'min_impurity_decrease': 0.0021, 'max_leaf_nodes': 174, 'max_depth': 16, 'criterion': 'gini'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sudeepkakarla/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "15 fits failed out of a total of 2500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sudeepkakarla/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/sudeepkakarla/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/sudeepkakarla/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 250, in fit\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/sudeepkakarla/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.82488631 0.77844887 0.83304336 0.83238518 0.843308   0.84398494\n",
      " 0.82488631 0.83230559 0.83304336 0.84574416 0.85537012 0.82615474\n",
      " 0.82487201 0.84375272 0.84175724 0.84671052 0.84353534 0.82488631\n",
      " 0.82487201 0.82824565 0.82488631 0.82488631 0.83993016 0.85537012\n",
      " 0.82488631 0.82488631 0.83927765 0.83613826 0.8233934         nan\n",
      " 0.82566132 0.82507291 0.83987605 0.82488631 0.82488631 0.83024761\n",
      " 0.82768187 0.8280068  0.82627955 0.83581309 0.82487201 0.84499722\n",
      " 0.82627955 0.8278815  0.82627955 0.83589091 0.83222366 0.85145045\n",
      " 0.82488631 0.8365337  0.82488631 0.82932484 0.83581309 0.84673915\n",
      " 0.83148343 0.82841043 0.82488631 0.82447048 0.82514459 0.83024761\n",
      " 0.8280068  0.82985596 0.82488631 0.82488631 0.82869408 0.83026191\n",
      " 0.82648669 0.84101504 0.82829096 0.77844887 0.84740936 0.83494371\n",
      " 0.8317569  0.82488631 0.85502561 0.82488631 0.83247073 0.83304336\n",
      " 0.82488631 0.8447298  0.84926223 0.82488631 0.82488631 0.83847851\n",
      " 0.82488631 0.82488631 0.84658679 0.83793687 0.82817806 0.8312966\n",
      " 0.83987605 0.83581309 0.82487201 0.82488631 0.82488631 0.83571509\n",
      " 0.82488631 0.83364647 0.84679758 0.84679758 0.82488631 0.83247073\n",
      " 0.84679758 0.8280068  0.83589091 0.83202234 0.82488631 0.8317569\n",
      " 0.84708677 0.82488631 0.83304336 0.84649426 0.83231997 0.83916589\n",
      " 0.82514459 0.82488631 0.83012345 0.83097744 0.83819427 0.83581309\n",
      " 0.83151218 0.82488631 0.8403447  0.83581309 0.8389098  0.82488631\n",
      " 0.8319632  0.84721677 0.83133873 0.77844887 0.82586846 0.83581309\n",
      " 0.8280068  0.83189672 0.8520543  0.83201054 0.82488631 0.82488631\n",
      " 0.82488631 0.84428996 0.83077653 0.83403088 0.82507291 0.84276783\n",
      " 0.85302601 0.82970101 0.77844887 0.82627955 0.82514459 0.82488631\n",
      " 0.82488631 0.83024761 0.82514459 0.83397977 0.82488631 0.82712333\n",
      " 0.82514459 0.83247073 0.82488631 0.83989593 0.82530903 0.77844887\n",
      " 0.82916914 0.82829096 0.8351278  0.83583312 0.82488631 0.82488631\n",
      " 0.837797   0.82488631 0.82769013 0.83245423 0.83247073 0.82514459\n",
      " 0.83323469 0.83581309 0.82488631 0.82857656 0.8450845  0.82488631\n",
      " 0.82693172 0.84189061 0.8418381  0.82488631 0.82488631 0.82488631\n",
      " 0.82891217 0.82105221 0.8378961  0.82566132 0.82488631 0.83809356\n",
      " 0.82488631 0.83111446 0.84174215 0.82841043 0.83035102 0.84708677\n",
      " 0.85146379        nan 0.82488631 0.83317591 0.82488631 0.84025534\n",
      " 0.83480762 0.83585906 0.82904928 0.83948543 0.83304336 0.82488631\n",
      " 0.8280068  0.84805907 0.82488631 0.84708677 0.84318707 0.82488631\n",
      " 0.82586846 0.82488631 0.82488631 0.83302929 0.82970101 0.82488631\n",
      " 0.82488631 0.82488631 0.83149416 0.82855513 0.82627955 0.83773101\n",
      " 0.82488631 0.83024761 0.82488631 0.84276655 0.83026191 0.82514459\n",
      " 0.83581309 0.8278815  0.83987605 0.82600201 0.83987605 0.82488631\n",
      " 0.8280068  0.83727502 0.82488631 0.84318221 0.83556602 0.85537012\n",
      " 0.8312966  0.82488631 0.82841043 0.84330489 0.83667838 0.82514459\n",
      " 0.8266966  0.84649426 0.82514459 0.82488631 0.82488631 0.82488631\n",
      " 0.82634772 0.82488631 0.84704981 0.82514459 0.82855513 0.82488631\n",
      " 0.83581309 0.83987605 0.83077653 0.85295798 0.83286766 0.82488631\n",
      " 0.82488631 0.83927395 0.85847289 0.8278815  0.83727502 0.8360193\n",
      " 0.82488631 0.84457122 0.83127438 0.82488631 0.83494371 0.82488631\n",
      " 0.84006265 0.8317569  0.8280068  0.82488631 0.82488631 0.8409476\n",
      " 0.8352192  0.82488631 0.82488631 0.839311   0.84062648 0.83556602\n",
      " 0.83359878 0.83350364 0.77844887 0.82779601 0.82488631 0.84133442\n",
      " 0.82947064 0.77844887 0.83007246 0.83733553 0.84720734 0.83997506\n",
      " 0.83225259 0.77844887 0.83247073 0.82514459 0.77844887 0.83385719\n",
      " 0.83825509 0.8398029  0.84516036 0.82487201 0.82488631 0.82488631\n",
      " 0.82768187 0.82488631 0.84006265 0.82488631 0.83773101 0.83165887\n",
      " 0.82488631 0.82868301 0.82488631 0.82514459 0.82488631 0.83585906\n",
      " 0.82488631 0.82488631 0.8278815  0.82566392 0.84174457 0.82488631\n",
      " 0.82488631 0.85478457 0.84159921 0.83276454 0.82488631 0.82940688\n",
      " 0.83153266 0.83286766 0.77844887 0.84123466 0.77844887 0.83585906\n",
      " 0.82488631 0.83773101 0.85088341 0.83247073 0.82780296 0.82488631\n",
      " 0.83581309 0.83388911 0.82488631 0.82488631 0.82978518 0.83886618\n",
      " 0.83890348 0.8312966  0.8321326  0.82488631 0.82488631 0.83556602\n",
      " 0.77844887 0.8307031  0.82488631 0.83581309 0.8354221  0.84247516\n",
      " 0.82488631        nan 0.83247073 0.84221652 0.84422075 0.83987605\n",
      " 0.77844887 0.82488631 0.82488631 0.82488631 0.82488631 0.82488631\n",
      " 0.82488631 0.82488631 0.83304336 0.82488631 0.83719479 0.82488631\n",
      " 0.83589944 0.83225259 0.82488631 0.83267695 0.8280068  0.84494157\n",
      " 0.82488631 0.82488631 0.84004255 0.82487201 0.82488631 0.84067823\n",
      " 0.82488631 0.82627955 0.83433522 0.82488631 0.83301534 0.82488631\n",
      " 0.83783511 0.82487201 0.82488631 0.82488631 0.83556602 0.85302231\n",
      " 0.82712333 0.83212181 0.82488631 0.82488631 0.82488631 0.83987605\n",
      " 0.83222366 0.83987605 0.83927765 0.82769069 0.82763596 0.84823064\n",
      " 0.8354221  0.84395386 0.83585906 0.82488631 0.84133442 0.82488631\n",
      " 0.82488631 0.82488631 0.82937615 0.82488631 0.82488631 0.84133389\n",
      " 0.82488631 0.82488631 0.77844887 0.82488631 0.77844887 0.83035102\n",
      " 0.82627955 0.82768187 0.84133442 0.82488631 0.84740936 0.82488631\n",
      " 0.82842473 0.82488631 0.8372954  0.82488631 0.82488631 0.83585906\n",
      " 0.82488631 0.83980868 0.85224283 0.8280068  0.82488631 0.82487201\n",
      " 0.82488631 0.8280068  0.82488631 0.84494157 0.83942419 0.82875603\n",
      " 0.82514459 0.83304336 0.82488631 0.82488631 0.84679758 0.83024761\n",
      " 0.77844887 0.83662196 0.82488631 0.82488631 0.83581309 0.82488631\n",
      " 0.8280068  0.82298236 0.82488631 0.83019629 0.82488631 0.84253064\n",
      " 0.82488631 0.8317569  0.82488631 0.82926049 0.84395479 0.82488631\n",
      " 0.83856831 0.83946408 0.83672945 0.82488631 0.84472482 0.84384569\n",
      " 0.82841043 0.77844887]\n",
      "  warnings.warn(\n",
      "/Users/sudeepkakarla/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the train scores are non-finite: [0.82583601 0.77774902 0.84049152 0.8404174  0.85134877 0.85716222\n",
      " 0.82583601 0.84187651 0.84049152 0.8526681  0.86875616 0.83070565\n",
      " 0.82811686 0.85732642 0.86286533 0.86679961 0.88045468 0.82583601\n",
      " 0.82811686 0.85207356 0.82583601 0.82583601 0.85733303 0.86875616\n",
      " 0.82583601 0.82583601 0.85612207 0.84253019 0.84531173        nan\n",
      " 0.8280218  0.83803457 0.84373893 0.82583601 0.82583601 0.83524992\n",
      " 0.83955617 0.83272552 0.83082791 0.84238417 0.82814917 0.85231332\n",
      " 0.83082791 0.83301112 0.83082791 0.85435129 0.83905609 0.86486851\n",
      " 0.82583601 0.83996952 0.82583601 0.86993382 0.84238417 0.85676346\n",
      " 0.83617732 0.83235552 0.82583601 0.85440082 0.82851932 0.83524992\n",
      " 0.83272552 0.85609445 0.82583601 0.82583601 0.83457867 0.83261379\n",
      " 0.83261694 0.85377908 0.84935738 0.77774902 0.85400728 0.84140665\n",
      " 0.83864725 0.82583601 0.87985013 0.82583601 0.83861476 0.84049152\n",
      " 0.82583601 0.86258614 0.8678224  0.82583601 0.82583601 0.86403712\n",
      " 0.82583601 0.82583601 0.86534297 0.86166173 0.86268563 0.83504222\n",
      " 0.84373893 0.84238417 0.82811686 0.82583601 0.82583601 0.86082638\n",
      " 0.82583601 0.83978321 0.85496571 0.8548865  0.82583601 0.83861476\n",
      " 0.8548865  0.83272552 0.85435129 0.85675788 0.82583601 0.83864725\n",
      " 0.85513746 0.82583601 0.84049152 0.85515207 0.8394209  0.86168885\n",
      " 0.82851932 0.82583601 0.83384655 0.86138074 0.86427125 0.84238417\n",
      " 0.83903085 0.82583601 0.87081112 0.84238417 0.84840174 0.82583601\n",
      " 0.86825306 0.85605656 0.83647977 0.77774902 0.83087084 0.84238417\n",
      " 0.83272552 0.86105736 0.87198295 0.85338453 0.82583601 0.82583601\n",
      " 0.82583601 0.84936169 0.83461817 0.85671621 0.83772011 0.85487675\n",
      " 0.87296975 0.83672211 0.77774902 0.83082791 0.82851932 0.82583601\n",
      " 0.82583601 0.83506881 0.82851932 0.85884788 0.82583601 0.84850853\n",
      " 0.82848866 0.83861476 0.82583601 0.84957198 0.84774249 0.77774902\n",
      " 0.83423412 0.84935738 0.85985284 0.84419188 0.82583601 0.82583601\n",
      " 0.84874326 0.82583601 0.83061584 0.85009702 0.83861476 0.82851932\n",
      " 0.85224366 0.84238417 0.82583601 0.84022157 0.85286541 0.82583601\n",
      " 0.85017143 0.84767897 0.84834074 0.82583601 0.82583601 0.82583601\n",
      " 0.83448741 0.86111985 0.85722785 0.82799056 0.82583601 0.84804305\n",
      " 0.82583601 0.85587451 0.849096   0.83235552 0.83667334 0.85516646\n",
      " 0.87238195        nan 0.82583601 0.83923703 0.82583601 0.87298428\n",
      " 0.84214072 0.84341642 0.83272057 0.86304257 0.84049152 0.82583601\n",
      " 0.83272552 0.85849603 0.82583601 0.85516646 0.86259598 0.82583601\n",
      " 0.83080794 0.82583601 0.82583601 0.83954919 0.83672211 0.82583601\n",
      " 0.82583601 0.82583601 0.86160502 0.83219021 0.83082791 0.84800719\n",
      " 0.82583601 0.83524992 0.82583601 0.84810619 0.83261379 0.82851932\n",
      " 0.84238417 0.83301112 0.84373893 0.86565203 0.84373893 0.82583601\n",
      " 0.83272552 0.84616532 0.82583601 0.85390339 0.8428255  0.86875616\n",
      " 0.83504222 0.82583601 0.83235552 0.91506869 0.85673595 0.82851932\n",
      " 0.84923667 0.85498969 0.82851932 0.82583601 0.82583601 0.82583601\n",
      " 0.84140421 0.82583601 0.86978719 0.82851932 0.83219021 0.82583601\n",
      " 0.84238417 0.84373893 0.83461817 0.86703641 0.83780297 0.82583601\n",
      " 0.82583601 0.86584822 0.87369714 0.83301112 0.84616532 0.84408099\n",
      " 0.82583601 0.85610219 0.83406008 0.82583601 0.84140665 0.82583601\n",
      " 0.84773699 0.83864725 0.83272552 0.82583601 0.82583601 0.84559228\n",
      " 0.84244045 0.82583601 0.82583601 0.8701549  0.84469235 0.8428255\n",
      " 0.84367198 0.85731114 0.77774902 0.85189012 0.82583601 0.84540251\n",
      " 0.85092494 0.77774902 0.83446533 0.87098512 0.86477095 0.84827272\n",
      " 0.83899004 0.77774902 0.83861476 0.82851932 0.77774902 0.86413551\n",
      " 0.84786933 0.86284149 0.85477917 0.82814917 0.82583601 0.82583601\n",
      " 0.83955617 0.82583601 0.84773699 0.82583601 0.84800719 0.85582446\n",
      " 0.82583601 0.83269501 0.82583601 0.82851932 0.82583601 0.84341642\n",
      " 0.82583601 0.82583601 0.83301112 0.84892694 0.86629258 0.82583601\n",
      " 0.82583601 0.86951919 0.84942021 0.86276097 0.82583601 0.83451486\n",
      " 0.83690262 0.83780297 0.77774902 0.85036837 0.77774902 0.84341642\n",
      " 0.82583601 0.84818137 0.89376687 0.83861476 0.85716496 0.82583601\n",
      " 0.84238417 0.85712058 0.82583601 0.82583601 0.85375413 0.85969102\n",
      " 0.85039497 0.83504222 0.88501966 0.82583601 0.82583601 0.8428255\n",
      " 0.77774902 0.85121502 0.82583601 0.84238417 0.84476624 0.86006197\n",
      " 0.82583601        nan 0.83861476 0.84752902 0.85141261 0.84373893\n",
      " 0.77774902 0.82583601 0.82583601 0.82583601 0.82583601 0.82583601\n",
      " 0.82583601 0.82583601 0.84049152 0.82583601 0.86008307 0.82583601\n",
      " 0.85663978 0.83899004 0.82583601 0.84031158 0.83272552 0.85313375\n",
      " 0.82583601 0.82583601 0.84923917 0.82814917 0.82583601 0.87312944\n",
      " 0.82583601 0.83082791 0.83790805 0.82583601 0.83765142 0.82583601\n",
      " 0.85602064 0.82814917 0.82583601 0.82583601 0.8428255  0.87760488\n",
      " 0.84850853 0.85124742 0.82583601 0.82583601 0.82583601 0.84373893\n",
      " 0.83905609 0.84373893 0.85612207 0.84838487 0.85170765 0.86356014\n",
      " 0.84476624 0.87962985 0.84341642 0.82583601 0.84540251 0.82583601\n",
      " 0.82583601 0.82583601 0.83455062 0.82583601 0.82583601 0.85195024\n",
      " 0.82583601 0.82583601 0.77774902 0.82583601 0.77774902 0.83660998\n",
      " 0.83082791 0.83955617 0.84540251 0.82583601 0.85400728 0.82583601\n",
      " 0.83004236 0.82583601 0.8567572  0.82583601 0.82583601 0.84341642\n",
      " 0.82583601 0.84628998 0.8728966  0.83272552 0.82583601 0.82814917\n",
      " 0.82583601 0.83272552 0.82583601 0.85318962 0.84433678 0.84193843\n",
      " 0.82851932 0.84049152 0.82583601 0.82583601 0.85496571 0.83506881\n",
      " 0.77774902 0.86212926 0.82583601 0.82583601 0.84238417 0.82583601\n",
      " 0.83272552 0.8459431  0.82583601 0.83571626 0.82583601 0.84850985\n",
      " 0.82583601 0.83864725 0.82583601 0.83397202 0.85661873 0.82583601\n",
      " 0.8437397  0.86026099 0.8567572  0.82583601 0.85951785 0.85776002\n",
      " 0.83235552 0.77774902]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"precision\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(1,80),  \n",
    "    'min_samples_leaf': np.arange(1,50),\n",
    "    'min_impurity_decrease': np.arange(0.0001, 0.01, 0.0005),\n",
    "    'max_leaf_nodes': np.arange(5, 200), \n",
    "    'max_depth': np.arange(1,50), \n",
    "    'criterion': ['entropy', 'gini'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "rand_search = RandomizedSearchCV(estimator = dtree, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dc8a700",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_matrix = confusion_matrix(y_test, rand_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"Random search\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "492cdf27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9072 candidates, totalling 45360 fits\n",
      "The best precision score is 0.8470330066484271\n",
      "... with parameters: {'criterion': 'entropy', 'max_depth': 15, 'max_leaf_nodes': 162, 'min_impurity_decrease': 0.0048, 'min_samples_leaf': 10, 'min_samples_split': 30}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"precision\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(30,36),  \n",
    "    'min_samples_leaf': np.arange(6,12),\n",
    "    'min_impurity_decrease': np.arange(0.0048, 0.0054, 0.0001),\n",
    "    'max_leaf_nodes': np.arange(162,168), \n",
    "    'max_depth': np.arange(15,21), \n",
    "    'criterion': ['entropy'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(estimator = dtree, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd2245ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"Grid search\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19342460",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_poly_model = SVC(kernel=\"poly\", degree=3, coef0=1, C=10,probability=True)\n",
    "_ = svm_poly_model.fit(X_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c97f064b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_preds = svm_poly_model.predict(X_test)\n",
    "c_matrix = confusion_matrix(y_test, model_preds)\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"poly svm\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80955dca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>Random search</td>\n",
       "      <td>0.847235</td>\n",
       "      <td>0.866290</td>\n",
       "      <td>0.849492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.837937</td>\n",
       "      <td>Grid search</td>\n",
       "      <td>0.846298</td>\n",
       "      <td>0.856874</td>\n",
       "      <td>0.847300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.855839</td>\n",
       "      <td>poly svm</td>\n",
       "      <td>0.867854</td>\n",
       "      <td>0.883239</td>\n",
       "      <td>0.869323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Precision          model  Accuracy    Recall        F1\n",
       "0   0.833333  Random search  0.847235  0.866290  0.849492\n",
       "0   0.837937    Grid search  0.846298  0.856874  0.847300\n",
       "0   0.855839       poly svm  0.867854  0.883239  0.869323"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdd8181",
   "metadata": {},
   "outputs": [],
   "source": [
    "#When compared to the models that we present here, we can see precision is high in poly SVM. Accuracy is also seen high for SVM which is the better one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
